#朴素贝叶斯分类常用于文本分类，一般可应用于垃圾文本过滤、情感预测、推荐系统等
#朴素贝叶斯分类器的三个流程：准备阶段、训练阶段、应用阶段
#准备阶段：分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定
#训练阶段：输入是特征属性和训练样本，输出是分类器
#应用阶段：输入是分类器和新数据，输出是新数据的分类结果。

'''
高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布（人的身高，物体的长度）
多项式朴素贝叶斯：特征变量是离散变量，符合多项分布（一个单词出现的频数，单词的TF-IDF值）
伯努利朴素贝叶斯：特征变量是布尔变量，符合0/1分布（单词是否出现）

词频TF=（单词出现的次数）/（该文档的总单词数）
逆向文档频率IDF=log（文档总数）/（该单词出现的文档数+1）
TF-IDF = TF*IDF
'''

'''
#创建TfidfVectorizer类
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
tfidf_vec = TfidfVectorizer()
documents = ['this is the bayes document','this is the second second document','and the third one','is this the document']
tfidf_matrix = tfidf_vec.fit_transform(documents)
print('不重复的词：',tfidf_vec.get_feature_names())
print('每个单词的ID：',tfidf_vec.vocabulary_)
print('每个单词的tfidf值：',tfidf_matrix.toarray())
'''

import os
import warnings
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import metrics
import jieba

warnings.filterwarnings('ignore')

#切词
def cut_words(file_path):
    """
    对文本进行切词
    :param file_path: txt文本路径
    :return: 用空格分词的字符串
    """
    text_with_spaces = ''
    text=open(file_path, 'r', encoding='gb18030').read()
    textcut = jieba.cut(text)
    for word in textcut:
        text_with_spaces += word + ' '
    return text_with_spaces

def loadfile(file_dir, label):
    """
    将路径下的所有文件加载
    :param file_dir: 保存txt文件目录
    :param label: 文档标签
    :return: 分词后的文档列表和标签
    """
    file_list = os.listdir(file_dir)
    words_list = []
    labels_list = []
    for file in file_list:
        file_path = file_dir + '/' + file
        words_list.append(cut_words(file_path))
        labels_list.append(label)
    return words_list, labels_list

# 训练数据
train_words_list1, train_labels1 = loadfile('text_classification-master/text classification/train/女性', '女性')
train_words_list2, train_labels2 = loadfile('text_classification-master/text classification/train/体育', '体育')
train_words_list3, train_labels3 = loadfile('text_classification-master/text classification/train/文学', '文学')
train_words_list4, train_labels4 = loadfile('text_classification-master/text classification/train/校园', '校园')

train_words_list = train_words_list1 + train_words_list2 + train_words_list3 + train_words_list4
train_labels = train_labels1 + train_labels2 + train_labels3 + train_labels4

# 测试数据
test_words_list1, test_labels1 = loadfile('text_classification-master/text classification/test/女性', '女性')
test_words_list2, test_labels2 = loadfile('text_classification-master/text classification/test/体育', '体育')
test_words_list3, test_labels3 = loadfile('text_classification-master/text classification/test/文学', '文学')
test_words_list4, test_labels4 = loadfile('text_classification-master/text classification/test/校园', '校园')

test_words_list = test_words_list1 + test_words_list2 + test_words_list3 + test_words_list4
test_labels = test_labels1 + test_labels2 + test_labels3 + test_labels4

#加载停用词表
stop_words = open('text_classification-master/text classification/stop/stopword.txt', 'r', encoding='utf-8').read()
stop_words = stop_words.encode('utf-8').decode('utf-8-sig') # 列表头部\ufeff处理
stop_words = stop_words.split('\n') # 根据分隔符分隔

# 计算单词权重
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)  #创建TfidfVectorizer类
# 上面fit过了，这里transform
train_features = tf.fit_transform(train_words_list)

# 多项式贝叶斯分类器
from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)

#用生成的分类器做预测
test_features = tf.transform(test_words_list)
predicted_labels=clf.predict(test_features)

# 计算准确率
print('准确率为：', metrics.accuracy_score(test_labels, predicted_labels))
